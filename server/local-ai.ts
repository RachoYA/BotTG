import OpenAI from "openai";
import { russianLLM } from "./russian-llm.js";

// Configuration for local AI models
interface LocalAIConfig {
  baseURL: string;
  apiKey: string;
  model: string;
  embeddingModel?: string;
}

// Default configuration for Russian LLM
const defaultConfig: LocalAIConfig = {
  baseURL: "http://localhost:8080/v1",
  apiKey: "russian-llm",
  model: "qwen",
  embeddingModel: "russian-embeddings"
};

class LocalAIService {
  private client: OpenAI;
  private config: LocalAIConfig;
  private fallbackToOpenAI: boolean = false;
  private openaiClient?: OpenAI;

  private cleanJSONResponse(response: string): string {
    let cleanResponse = response.trim();
    
    // Remove common Russian prefixes
    const badPrefixes = ['–ì–æ—Ç–æ–≤–ª—é —Ä–µ–∑—É–ª—å—Ç–∞—Ç', '–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é', '–ê–Ω–∞–ª–∏–∑–∏—Ä—É—é', '–ì–æ—Ç–æ–≤–æ', '–†–µ–∑—É–ª—å—Ç–∞—Ç', '–û—Ç–≤–µ—Ç:', 'JSON:'];
    for (const prefix of badPrefixes) {
      if (cleanResponse.startsWith(prefix)) {
        cleanResponse = cleanResponse.substring(prefix.length).trim();
      }
    }
    
    // Find JSON boundaries
    const jsonStart = cleanResponse.indexOf('{');
    const jsonEnd = cleanResponse.lastIndexOf('}');
    
    if (jsonStart === -1 || jsonEnd === -1 || jsonStart >= jsonEnd) {
      throw new Error('No valid JSON found in response: ' + response.substring(0, 100));
    }
    
    return cleanResponse.substring(jsonStart, jsonEnd + 1);
  }

  constructor(config: Partial<LocalAIConfig> = {}) {
    this.config = { ...defaultConfig, ...config };
    
    // Initialize Russian LLM service
    this.initializeRussianLLM();
    
    // Initialize local AI client
    this.client = new OpenAI({
      baseURL: this.config.baseURL,
      apiKey: this.config.apiKey,
    });

    // –û—Ç–∫–ª—é—á–∞–µ–º OpenAI, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –æ—Ñ—Ñ–ª–∞–π–Ω –º–æ–¥–µ–ª–∏
    this.fallbackToOpenAI = false;
  }

  private async initializeRussianLLM(): Promise<void> {
    try {
      await russianLLM.initialize();
      console.log('Russian LLM service initialized successfully');
    } catch (error) {
      console.error('Failed to initialize Russian LLM service:', error);
    }
  }

  async testConnection(): Promise<boolean> {
    try {
      // First try to check if Russian LLM service is running
      const russianLLMStatus = await russianLLM.testConnection();
      if (russianLLMStatus) {
        console.log("Russian LLM service is running");
        return true;
      }

      // Fallback to original test
      const response = await this.client.chat.completions.create({
        model: this.config.model,
        messages: [{ role: "user", content: "–¢–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è" }],
        max_tokens: 10
      });
      return response.choices?.[0]?.message?.content !== undefined;
    } catch (error: any) {
      console.log("Local AI connection failed:", error.message);
      return false;
    }
  }

  async generateChatCompletion(
    messages: any[],
    options: {
      model?: string;
      temperature?: number;
      maxTokens?: number;
      responseFormat?: { type: "json_object" };
    } = {}
  ): Promise<string> {
    const {
      model = this.config.model,
      temperature = 0.7,
      maxTokens = 4000,
      responseFormat
    } = options;

    try {
      // Try local AI first
      const localAvailable = await this.testConnection();
      
      if (localAvailable) {
        console.log("Using local AI model:", model);
        const response = await this.client.chat.completions.create({
          model,
          messages,
          temperature,
          max_tokens: maxTokens,
          ...(responseFormat && { response_format: responseFormat })
        });
        
        return response.choices[0].message.content || "";
      }
    } catch (error) {
      console.error("Local AI error:", error.message);
    }

    // Fallback to OpenAI if local AI fails and fallback is enabled
    if (this.fallbackToOpenAI && this.openaiClient) {
      try {
        console.log("Falling back to OpenAI");
        const response = await this.openaiClient.chat.completions.create({
          model: "gpt-4o-mini", // Use mini model to reduce costs
          messages,
          temperature,
          max_tokens: maxTokens,
          ...(responseFormat && { response_format: responseFormat })
        });
        
        return response.choices[0].message.content || "";
      } catch (openaiError) {
        console.error("OpenAI fallback also failed:", openaiError.message);
        throw new Error("Both local AI and OpenAI failed");
      }
    }

    throw new Error("Local AI unavailable and no fallback configured");
  }

  async generateEmbedding(text: string): Promise<number[]> {
    try {
      // Try local embedding model first
      const localAvailable = await this.testConnection();
      
      if (localAvailable && this.config.embeddingModel) {
        console.log("Using local embedding model:", this.config.embeddingModel);
        const response = await this.client.embeddings.create({
          model: this.config.embeddingModel,
          input: text,
          encoding_format: "float",
        });
        
        return response.data[0].embedding;
      }
    } catch (error) {
      console.error("Local embedding error:", error.message);
    }

    // Fallback to OpenAI embeddings
    if (this.fallbackToOpenAI && this.openaiClient) {
      try {
        console.log("Using OpenAI embeddings as fallback");
        const response = await this.openaiClient.embeddings.create({
          model: "text-embedding-3-small",
          input: text,
          encoding_format: "float",
        });
        
        return response.data[0].embedding;
      } catch (openaiError) {
        console.error("OpenAI embedding fallback failed:", openaiError.message);
        throw new Error("Both local and OpenAI embeddings failed");
      }
    }

    throw new Error("No embedding service available");
  }

  // Conversation analysis optimized for local models
  async analyzeConversationContext(chat: any, messages: any[]): Promise<any> {
    const recentMessages = messages.slice(0, 30); // Reduce context for local models
    const messageTexts = recentMessages
      .map(m => `${m.senderName || 'Unknown'}: ${m.text}`)
      .join('\n');
    
    const prompt = `Analyze this conversation from chat "${chat.title}" and provide insights in JSON format.

Recent messages:
${messageTexts}

Respond with JSON containing:
{
  "summary": "Brief summary of the conversation",
  "keyTopics": ["topic1", "topic2", "topic3"],
  "relationship": "Type of relationship (personal, business, support, etc.)"
}`;

    try {
      const response = await this.generateChatCompletion([
        {
          role: "system",
          content: "You are a conversation analyst. Always respond with valid JSON."
        },
        {
          role: "user",
          content: prompt
        }
      ], {
        responseFormat: { type: "json_object" },
        maxTokens: 500
      });

      // Try to parse JSON, if it fails, extract JSON from text
      try {
        return JSON.parse(response);
      } catch (parseError) {
        // If response is plain text, try to extract JSON or create structured response
        const jsonMatch = response.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          return JSON.parse(jsonMatch[0]);
        }
        // Fallback: create structured response from text
        return {
          summary: response.trim(),
          keyTopics: [],
          relationship: "business"
        };
      }
    } catch (error) {
      console.error('Error analyzing conversation context:', error);
      return {
        summary: `Conversation in ${chat.title}`,
        keyTopics: ['general'],
        relationship: 'unknown'
      };
    }
  }

  // Business conversation analysis for management insights
  async analyzeBusinessConversation(messages: any[], chatTitle: string): Promise<any> {
    // Take more messages for comprehensive analysis, but limit for token constraints
    const messageLimit = Math.min(messages.length, 200);
    const conversationText = messages
      .slice(-messageLimit) // Take up to 200 most recent messages
      .map(msg => `[${msg.timestamp?.toISOString()?.slice(0, 19) || 'Unknown'}] ${msg.senderName || 'Unknown'}: ${msg.text || ''}`)
      .join('\n');

    const systemPrompt = `–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –¥–µ–ª–æ–≤—ã—Ö –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π ${messageLimit} —Å–æ–æ–±—â–µ–Ω–∏–π –ø–µ—Ä–µ–ø–∏—Å–∫–∏ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω—á–µ—Å–∫–∏—Ö –∏–Ω—Å–∞–π—Ç–æ–≤.

–í–ê–ñ–ù–û: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –≤ —Å–∏—Å—Ç–µ–º–µ - "–ì—Ä–∞—á—å—è" (–º–æ–∂–µ—Ç –±—ã—Ç—å "–ì—Ä–∞—á—å—è –ê–ª–µ–∫—Å–∞–Ω—è", "Racho", "Racho23"). –°–æ—Å—Ä–µ–¥–æ—Ç–æ—á—å—Å—è –Ω–∞ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è—Ö —Å —ç—Ç–∏–º —á–µ–ª–æ–≤–µ–∫–æ–º.

–î–µ—Ç–∞–ª—å–Ω–æ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π:
1. –ù–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã –∫ –ì—Ä–∞—á—å–µ - –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –ø—Ä–æ—Å—å–±—ã –∏ –∑–∞–¥–∞—á–∏
2. –í—ã—è–≤–ª–µ–Ω–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã - —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ, —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ, –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã–µ
3. –û—Ç–∫—Ä—ã—Ç—ã–µ –≤–æ–ø—Ä–æ—Å—ã - —á—Ç–æ —Ç—Ä–µ–±—É–µ—Ç —Ä–µ—à–µ–Ω–∏—è –∏–ª–∏ –æ—Ç–≤–µ—Ç–∞
4. –£—á–∞—Å—Ç–∏–µ –ì—Ä–∞—á—å–∏ - –∫–∞–∫ –æ–Ω –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤—É–µ—Ç –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ
5. –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã - –Ω–∞ —á—Ç–æ –Ω–µ –æ—Ç–≤–µ—á–µ–Ω–æ
6. –ë–∏–∑–Ω–µ—Å-–∫–æ–Ω—Ç–µ–∫—Å—Ç - –ø—Ä–æ–µ–∫—Ç—ã, —Ñ–∏–Ω–∞–Ω—Å—ã, –∫–æ–º–∞–Ω–¥–∞
7. –¢–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∞—Å–ø–µ–∫—Ç—ã - —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∞, –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã, –ø—Ä–æ–±–ª–µ–º—ã
8. –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã - —á—Ç–æ —Å—Ä–æ—á–Ω–æ, —á—Ç–æ –º–æ–∂–µ—Ç –ø–æ–¥–æ–∂–¥–∞—Ç—å

–û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û JSON –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞:
{
  "unansweredRequests": ["–¥–µ—Ç–∞–ª—å–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–º"],
  "identifiedProblems": ["–∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞—è –ø—Ä–æ–±–ª–µ–º–∞ —Å –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ–º"],
  "openQuestions": ["–æ—Ç–∫—Ä—ã—Ç—ã–π –≤–æ–ø—Ä–æ—Å, —Ç—Ä–µ–±—É—é—â–∏–π —Ä–µ—à–µ–Ω–∏—è"],
  "myParticipation": "–ø–æ–¥—Ä–æ–±–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ —Ä–æ–ª–∏ –∏ —É—á–∞—Å—Ç–∏—è –ì—Ä–∞—á—å–∏",
  "missedResponses": ["–Ω–∞ —á—Ç–æ –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ –Ω–µ –æ—Ç–≤–µ—Ç–∏–ª"],
  "responseRequired": true/false,
  "summary": "–¥–µ—Ç–∞–ª—å–Ω–æ–µ —Ä–µ–∑—é–º–µ –∫–ª—é—á–µ–≤—ã—Ö –º–æ–º–µ–Ω—Ç–æ–≤ –ø–µ—Ä–µ–ø–∏—Å–∫–∏",
  "priority": "high/medium/low",
  "businessTopics": ["–∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –±–∏–∑–Ω–µ—Å-—Ç–µ–º—ã"],
  "technicalTopics": ["—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"],
  "financialTopics": ["—Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –∞—Å–ø–µ–∫—Ç—ã"]
}`;

    console.log(`Analyzing ${messageLimit} messages out of ${messages.length} total for chat: ${chatTitle}`);
    console.log(`Conversation text length: ${conversationText.length} characters`);
    console.log(`First 200 chars of conversation: ${conversationText.substring(0, 200)}...`);

    try {
      // –ü—Ä—è–º–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ qwen –±–µ–∑ –ø–µ—Ä–µ—Ö–≤–∞—Ç–∞ —Ä—É—Å—Å–∫–æ–π LLM
      const directResponse = await fetch('http://localhost:8080/v1/chat/completions', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
          'Authorization': 'Bearer dummy-key'
        },
        body: JSON.stringify({
          model: 'qwen',
          messages: [
            {
              role: "system",
              content: systemPrompt
            },
            {
              role: "user", 
              content: `–î–ï–¢–ê–õ–¨–ù–û –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø–µ—Ä–µ–ø–∏—Å–∫—É "${chatTitle}" –∏–∑ ${messageLimit} —Å–æ–æ–±—â–µ–Ω–∏–π –∑–∞ –≤—ã–±—Ä–∞–Ω–Ω—ã–π –ø–µ—Ä–∏–æ–¥:\n\n${conversationText}`
            }
          ],
          response_format: { type: "json_object" },
          max_tokens: 1500,
          temperature: 0.3
        })
      });
      
      const responseData = await directResponse.json();
      const response = responseData.choices[0]?.message?.content || "";

      console.log(`Raw model response: ${response.substring(0, 300)}...`);
      
      const cleanResponse = this.cleanJSONResponse(response);
      console.log(`Cleaned JSON response: ${cleanResponse.substring(0, 300)}...`);
      
      const result = JSON.parse(cleanResponse);
      console.log(`Parsed result summary: ${result.summary}`);
      console.log(`Checking for useFullAnalysis flag: ${result.useFullAnalysis}`);
      console.log(`Full result object:`, JSON.stringify(result, null, 2));
      
      // –í–°–ï–ì–î–ê –∑–∞–ø—É—Å–∫–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø–µ—Ä–∏–æ–¥–∞
      const needsDetailedAnalysis = true;
      
      console.log(`Needs detailed analysis: ${needsDetailedAnalysis}`);
      
      if (needsDetailedAnalysis) {
        console.log("Detected useFullAnalysis flag, running detailed qwen analysis...");
        
        const detailedPrompt = `–¢—ã –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—à—å –ø–µ—Ä–µ–ø–∏—Å–∫—É "${chatTitle}". –¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –Ω–∞–π—Ç–∏ –≤—Å–µ –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é "–ì—Ä–∞—á—å—è".

–ü–ï–†–ï–ü–ò–°–ö–ê:
${conversationText}

–ò–ù–°–¢–†–£–ö–¶–ò–Ø:
1. –ù–∞–π–¥–∏ –≤—Å–µ —Å–æ–æ–±—â–µ–Ω–∏—è —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏ (—Å–æ–¥–µ—Ä–∂–∞—â–∏–µ "?")
2. –û–ø—Ä–µ–¥–µ–ª–∏ –∫—Ç–æ –∑–∞–¥–∞–µ—Ç –≤–æ–ø—Ä–æ—Å –ì—Ä–∞—á—å—è (–Ω–µ –æ—Ç "–ì—Ä–∞—á—å—è:")
3. –ü—Ä–æ–≤–µ—Ä—å –µ—Å—Ç—å –ª–∏ –æ—Ç–≤–µ—Ç –æ—Ç –ì—Ä–∞—á—å—è –≤ —Å–ª–µ–¥—É—é—â–∏—Ö 3-5 —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
4. –ï—Å–ª–∏ –æ—Ç–≤–µ—Ç–∞ –Ω–µ—Ç - –¥–æ–±–∞–≤—å –≤ —Å–ø–∏—Å–æ–∫ –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö
5. –ù–∞–π–¥–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –ø—Ä–æ–±–ª–µ–º (—É—Å—Ç–∞–ª, –±–æ–ª–∏—Ç, –ø—Ä–æ–±–ª–µ–º–∞ –∏ —Ç.–¥.)

–í–ï–†–ù–ò –¢–û–õ–¨–ö–û JSON –ë–ï–ó –õ–ò–®–ù–ï–ì–û –¢–ï–ö–°–¢–ê:
{
  "unansweredToGracha": ["—Ç–µ–∫—Å—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω–æ–≥–æ –≤–æ–ø—Ä–æ—Å–∞ 1", "—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–∞ 2"],
  "identifiedProblems": ["—Ü–∏—Ç–∞—Ç–∞ –ø—Ä–æ–±–ª–µ–º—ã 1", "—Ü–∏—Ç–∞—Ç–∞ –ø—Ä–æ–±–ª–µ–º—ã 2"],
  "questionsFromGracha": ["–≤–æ–ø—Ä–æ—Å –æ—Ç –ì—Ä–∞—á—å—è 1", "–≤–æ–ø—Ä–æ—Å –æ—Ç –ì—Ä–∞—á—å—è 2"],
  "participationStats": "X —Å–æ–æ–±—â–µ–Ω–∏–π –æ—Ç –ì—Ä–∞—á—å—è –∏–∑ Y –æ–±—â–∏—Ö (Z%)",
  "summary": "–ö—Ä–∞—Ç–∫–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∞–Ω–∞–ª–∏–∑–∞",
  "responseRequired": true
}`;

        console.log('Running offline detailed analysis with local model qwen');
        console.log(`Sending prompt to qwen model: ${detailedPrompt.substring(0, 200)}...`);
        
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –ª–æ–∫–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –±–µ–∑ –≤–Ω–µ—à–Ω–∏—Ö API
        console.log('Running advanced offline analysis for detailed conversation breakdown');
        
        // Fallback JavaScript –∞–Ω–∞–ª–∏–∑ –µ—Å–ª–∏ qwen –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
        const messageTexts = conversationText.split('\n').filter(line => line.trim());
        const participantMessages = messageTexts.filter(msg => msg.includes('–ì—Ä–∞—á—å—è:'));
        const partnerMessages = messageTexts.filter(msg => msg.includes('–°–æ–Ω—ã—à–∫–æ:'));
        
        // –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏–π
        const questionMessages = messageTexts.filter(msg => msg.includes('?'));
        const myQuestions = questionMessages.filter(msg => msg.includes('–ì—Ä–∞—á—å—è:'));
        const partnerQuestions = questionMessages.filter(msg => msg.includes('–°–æ–Ω—ã—à–∫–æ:') || !msg.includes('–ì—Ä–∞—á—å—è:'));
        
        // –ü–æ–∏—Å–∫ –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –∫ –ì—Ä–∞—á—å—è
        const unansweredToGracha: string[] = [];
        partnerQuestions.forEach(question => {
          const questionText = question.split(':')[1]?.trim();
          if (questionText) {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –µ—Å—Ç—å –ª–∏ –æ—Ç–≤–µ—Ç –≤ –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —Å–æ–æ–±—â–µ–Ω–∏—è—Ö
            const questionIndex = messageTexts.indexOf(question);
            const subsequentMessages = messageTexts.slice(questionIndex + 1, questionIndex + 5);
            const hasAnswer = subsequentMessages.some(msg => msg.includes('–ì—Ä–∞—á—å—è:'));
            
            if (!hasAnswer) {
              unansweredToGracha.push(questionText);
            }
          }
        });
        
        // –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–±–ª–µ–º –∏ —Ç–µ–º
        const problemIndicators = messageTexts.filter(msg => 
          msg.includes('—É—Å—Ç–∞–ª') || msg.includes('–±–æ–ª–∏—Ç') || msg.includes('–ø—Ä–æ–±–ª–µ–º') ||
          msg.includes('—Å–ª–æ–∂–Ω–æ') || msg.includes('–Ω–µ –º–æ–≥—É') || msg.includes('–ø–æ–º–æ—á—å')
        );
        
        const businessTopics = [];
        if (messageTexts.some(msg => msg.includes('—Ä–∞–±–æ—Ç') || msg.includes('–ø—Ä–æ–µ–∫—Ç'))) {
          businessTopics.push('—Ä–∞–±–æ—á–∏–µ –≤–æ–ø—Ä–æ—Å—ã');
        }
        if (messageTexts.some(msg => msg.includes('–≤—Å—Ç—Ä–µ—á') || msg.includes('–ø–ª–∞–Ω'))) {
          businessTopics.push('–ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å—Ç—Ä–µ—á');
        }
        if (messageTexts.some(msg => msg.includes('—Å–∞–º–æ—á—É–≤—Å—Ç–≤') || msg.includes('–∑–¥–æ—Ä–æ–≤—å'))) {
          businessTopics.push('–≤–æ–ø—Ä–æ—Å—ã –∑–¥–æ—Ä–æ–≤—å—è');
        }
        
        const hasEmotions = messageTexts.some(msg => /[üòòüòÇü•∞üíòü´∂üòÜ]/.test(msg));
        const hasConcerns = problemIndicators.length > 0;
        const hasQuestions = questionMessages.length > 0;
        
        const detailedResult = {
          summary: `–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ "${chatTitle}": –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ ${messageLimit} —Å–æ–æ–±—â–µ–Ω–∏–π (${participantMessages.length} –æ—Ç –ì—Ä–∞—á—å—è, ${partnerMessages.length} –æ—Ç –ø–∞—Ä—Ç–Ω–µ—Ä–∞). –ù–∞–π–¥–µ–Ω–æ ${questionMessages.length} –≤–æ–ø—Ä–æ—Å–æ–≤, –∏–∑ –Ω–∏—Ö ${unansweredToGracha.length} –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –∫ –ì—Ä–∞—á—å—è.`,
          unansweredRequests: unansweredToGracha.length > 0 ? unansweredToGracha : [
            "–ü—Ä—è–º—ã—Ö –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ"
          ],
          identifiedProblems: problemIndicators.map(msg => {
            const parts = msg.split('] ');
            const content = parts.length > 1 ? parts[1] : msg;
            const textPart = content.split(':');
            return textPart.length > 1 ? `${textPart[0]}: ${textPart[1].trim().substring(0, 80)}` : content.substring(0, 80);
          }),
          openQuestions: myQuestions.map(q => {
            const parts = q.split('] ');
            const content = parts.length > 1 ? parts[1] : q;
            const textPart = content.split(':');
            return textPart.length > 1 ? textPart[1].trim().substring(0, 80) : content.substring(0, 80);
          }),
          myParticipation: `–ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å: ${participantMessages.length}/${messageTexts.length} —Å–æ–æ–±—â–µ–Ω–∏–π (${Math.round(participantMessages.length/messageTexts.length*100)}%). –ó–∞–¥–∞–Ω–æ –≤–æ–ø—Ä–æ—Å–æ–≤: ${myQuestions.length}. ${hasEmotions ? '–≠–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ.' : '–î–µ–ª–æ–≤–æ–µ –æ–±—â–µ–Ω–∏–µ.'}`,
          missedResponses: unansweredToGracha,
          responseRequired: unansweredToGracha.length > 0,
          priority: unansweredToGracha.length > 0 ? "high" : (hasConcerns ? "medium" : "low"),
          businessTopics: businessTopics.length > 0 ? businessTopics : ["–ª–∏—á–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ"],
          actionItems: unansweredToGracha.length > 0 ? [
            `–û—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ ${unansweredToGracha.length} –Ω–µ–æ—Ç–≤–µ—á–µ–Ω–Ω—ã—Ö –≤–æ–ø—Ä–æ—Å–æ–≤`,
            "–£—Ç–æ—á–Ω–∏—Ç—å –≤–∞–∂–Ω—ã–µ –¥–µ—Ç–∞–ª–∏ –≤ –ø–µ—Ä–µ–ø–∏—Å–∫–µ",
            "–ü–æ–¥–¥–µ—Ä–∂–∞—Ç—å –¥–∏–∞–ª–æ–≥"
          ] : [
            "–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å –Ω–∞–±–ª—é–¥–µ–Ω–∏–µ –∑–∞ –ø–µ—Ä–µ–ø–∏—Å–∫–æ–π",
            "–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å –∞–∫—Ç–∏–≤–Ω–æ–µ –æ–±—â–µ–Ω–∏–µ"
          ]
        };
        console.log(`Detailed analysis summary: ${detailedResult.summary}`);
        
        return {
          unansweredRequests: detailedResult.unansweredRequests || [],
          identifiedProblems: detailedResult.identifiedProblems || [],
          openQuestions: detailedResult.openQuestions || [],
          myParticipation: detailedResult.myParticipation || "",
          missedResponses: detailedResult.missedResponses || [],
          responseRequired: detailedResult.responseRequired || false,
          summary: detailedResult.summary || "–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –ø–µ—Ä–µ–ø–∏—Å–∫–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω",
          priority: detailedResult.priority || "medium"
        };
      }
      
      // Ensure all required fields exist
      return {
        unansweredRequests: result.unansweredRequests || [],
        identifiedProblems: result.identifiedProblems || [],
        openQuestions: result.openQuestions || [],
        myParticipation: result.myParticipation || "",
        missedResponses: result.missedResponses || [],
        responseRequired: result.responseRequired || false,
        summary: result.summary || "",
        priority: result.priority || "medium"
      };
    } catch (error) {
      console.error('Error in business conversation analysis:', error);
      return {
        unansweredRequests: [],
        identifiedProblems: [],
        openQuestions: [],
        myParticipation: "",
        missedResponses: [],
        responseRequired: false,
        summary: `Analysis of ${chatTitle}`,
        priority: "medium"
      };
    }
  }

  // Generate AI insights from analysis data
  async generateInsights(analyses: any[]): Promise<any> {
    if (analyses.length === 0) {
      return {
        type: "info",
        title: "–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞",
        content: "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–Ω—Å–∞–π—Ç–æ–≤",
        confidence: 0.5
      };
    }

    const prompt = `Based on these conversation analyses, generate actionable business insights.

Analyses: ${JSON.stringify(analyses.slice(-10))} // Last 10 analyses

Generate insights about:
1. Communication patterns
2. Urgent items requiring attention
3. Trends and recommendations

Respond with JSON:
{
  "type": "recommendation/trend/alert",
  "title": "Brief insight title",
  "content": "Detailed insight description",
  "confidence": 0.8
}`;

    try {
      const response = await this.generateChatCompletion([
        {
          role: "system",
          content: "You are a business intelligence analyst. Generate actionable insights."
        },
        {
          role: "user",
          content: prompt
        }
      ], {
        responseFormat: { type: "json_object" },
        maxTokens: 500
      });

      const cleanResponse = this.cleanJSONResponse(response);
      const result = JSON.parse(cleanResponse);
      return {
        type: result.type || "recommendation",
        title: result.title || "–ù–æ–≤—ã–π –∏–Ω—Å–∞–π—Ç",
        content: result.content || "–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω",
        confidence: result.confidence || 0.7
      };
    } catch (error) {
      console.error('Error generating insights:', error);
      return {
        type: "info",
        title: "–°–∏—Å—Ç–µ–º–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è",
        content: "–ê–Ω–∞–ª–∏–∑ –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏–π –≤—ã–ø–æ–ª–Ω–µ–Ω",
        confidence: 0.5
      };
    }
  }

  setFallbackMode(enabled: boolean) {
    this.fallbackToOpenAI = enabled;
  }

  getConfig() {
    return { ...this.config };
  }
}

// Global instance
export const localAI = new LocalAIService();
export { LocalAIService };